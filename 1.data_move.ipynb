{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 시드 설정\n",
    "RANDOM_SEED = 2024\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# AI-Hub의 낙상사고 위험동작 영상-센서 쌍 데이터를 다운로드 (https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=&topMenu=&aihubDataSe=data&dataSetSn=71641)\n",
    "# D:\\Falldown\\Dataset\\Original_Dataset를 다운로드 경로로 지정\n",
    "# 01.원천데이터 -> SourceData (폴더 이름 변경)\n",
    "# 02.라벨링데이터 -> LabelingData (폴더 이름 변경)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 파일 복사 함수\n",
    "def copy_files(destination, groups, group_dict, source=None):\n",
    "    moved_files = 0  # 이동된 파일 수\n",
    "    skipped_files = 0  # 건너뛴 파일 수\n",
    "    if not os.path.exists(destination):\n",
    "        os.makedirs(destination)\n",
    "\n",
    "    for group in tqdm(groups, desc=f\"{source}에서 {destination}로 파일 복사 중\"):\n",
    "        for file_path in group_dict[group]:\n",
    "            dest_path = os.path.join(destination, os.path.basename(file_path))\n",
    "            if os.path.exists(dest_path):  # 파일이 이미 존재하는 경우\n",
    "                skipped_files += 1\n",
    "                continue  # 복사하지 않고 건너뜀\n",
    "\n",
    "            shutil.copy(file_path, dest_path)\n",
    "            moved_files += 1\n",
    "\n",
    "    # 작업 결과 출력\n",
    "    print(f\"'{destination}'에 파일 복사 완료: {moved_files}개 파일 복사, {skipped_files}개 파일 건너뜀.\")\n",
    "    return moved_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Source 경로\n",
    "source_folders_N = glob.glob('D:\\\\Falldown\\\\Dataset\\\\Original_Dataset\\\\*\\\\SourceData\\\\Image\\\\N', recursive=True)\n",
    "source_folders_Y = glob.glob('D:\\\\Falldown\\\\Dataset\\\\Original_Dataset\\\\*\\\\SourceData\\\\Image\\\\Y', recursive=True)\n",
    "\n",
    "# Destination 경로\n",
    "base_dataset_dir = 'D:\\\\Falldown\\\\Dataset\\\\Sorted_Dataset'\n",
    "train_folder = os.path.join(base_dataset_dir, 'Train', 'Image')\n",
    "val_folder = os.path.join(base_dataset_dir, 'Val', 'Image')\n",
    "test_folder = os.path.join(base_dataset_dir, 'Test', 'Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n예시)\\nfile_groups = {\\n    \"00050_H_A_BY_C1\": [\\n        r\"D:\\\\Falldown\\\\Dataset\\\\Original_Dataset\\\\Training\\\\SourceData\\\\Image\\\\Y\\\\BY\\x0050_H_A_BY_C1\\x0050_H_A_BY_C1_I001.jpg\",\\n        r\"D:\\\\Falldown\\\\Dataset\\\\Original_Dataset\\\\Training\\\\SourceData\\\\Image\\\\Y\\\\BY\\x0050_H_A_BY_C1\\x0050_H_A_BY_C1_I002.jpg\"\\n    ]\\n}\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 그룹화\n",
    "file_groups_N = defaultdict(list)\n",
    "file_groups_Y = defaultdict(list)\n",
    "\n",
    "for folder in source_folders_N:\n",
    "    for file_path in glob.glob(f\"{folder}\\\\**\\\\*.jpg\", recursive=True):\n",
    "        group_key = os.path.basename(file_path).rsplit('_I', 1)[0]\n",
    "        file_groups_N[group_key].append(file_path)\n",
    "\n",
    "for folder in source_folders_Y:\n",
    "    for file_path in glob.glob(f\"{folder}\\\\**\\\\*.jpg\", recursive=True):\n",
    "        group_key = os.path.basename(file_path).rsplit('_I', 1)[0]\n",
    "        file_groups_Y[group_key].append(file_path)\n",
    "\n",
    "\"\"\"\n",
    "예시)\n",
    "file_groups = {\n",
    "    \"00050_H_A_BY_C1\": [\n",
    "        r\"D:\\Falldown\\Dataset\\Original_Dataset\\Training\\SourceData\\Image\\Y\\BY\\00050_H_A_BY_C1\\00050_H_A_BY_C1_I001.jpg\",\n",
    "        r\"D:\\Falldown\\Dataset\\Original_Dataset\\Training\\SourceData\\Image\\Y\\BY\\00050_H_A_BY_C1\\00050_H_A_BY_C1_I002.jpg\"\n",
    "    ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할 함수\n",
    "def split_data(file_groups, split_ratios):\n",
    "    group_keys = list(file_groups.keys())\n",
    "    random.shuffle(group_keys)\n",
    "\n",
    "    train_end = int(len(group_keys) * split_ratios[0])\n",
    "    val_end = train_end + int(len(group_keys) * split_ratios[1])\n",
    "\n",
    "    train_groups = group_keys[:train_end]\n",
    "    val_groups = group_keys[train_end:val_end]\n",
    "    test_groups = group_keys[val_end:]\n",
    "\n",
    "    return train_groups, val_groups, test_groups\n",
    "\n",
    "# Train, Validation, Test 비율\n",
    "SPLIT_RATIOS = (0.8, 0.1, 0.1)\n",
    "\n",
    "# N 데이터 분할\n",
    "train_groups_N, val_groups_N, test_groups_N = split_data(file_groups_N, SPLIT_RATIOS)\n",
    "\n",
    "# Y 데이터에서 사용할 그룹 키를 1/3로 줄이고 새롭게 그룹화\n",
    "reduced_y_keys = random.sample(list(file_groups_Y.keys()), len(file_groups_Y) // 3) # 전체 키 개수의 1/3만큼 랜덤으로 선택\n",
    "file_groups_Y = {key: file_groups_Y[key] for key in reduced_y_keys}\n",
    "\n",
    "# Y 데이터 분할\n",
    "train_groups_Y, val_groups_Y, test_groups_Y = split_data(file_groups_Y, SPLIT_RATIOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N 데이터에서 D:\\Falldown\\Dataset\\Sorted_Dataset\\Train\\Image로 파일 복사 중: 100%|██████████| 4089/4089 [05:19<00:00, 12.81it/s]\n",
      "N 데이터에서 D:\\Falldown\\Dataset\\Sorted_Dataset\\Val\\Image로 파일 복사 중: 100%|██████████| 511/511 [00:41<00:00, 12.34it/s]\n",
      "N 데이터에서 D:\\Falldown\\Dataset\\Sorted_Dataset\\Test\\Image로 파일 복사 중: 100%|██████████| 512/512 [00:42<00:00, 12.03it/s]\n",
      "Y 데이터에서 D:\\Falldown\\Dataset\\Sorted_Dataset\\Train\\Image로 파일 복사 중: 100%|██████████| 4076/4076 [06:04<00:00, 11.18it/s]\n",
      "Y 데이터에서 D:\\Falldown\\Dataset\\Sorted_Dataset\\Val\\Image로 파일 복사 중: 100%|██████████| 509/509 [00:44<00:00, 11.48it/s]\n",
      "Y 데이터에서 D:\\Falldown\\Dataset\\Sorted_Dataset\\Test\\Image로 파일 복사 중: 100%|██████████| 511/511 [00:44<00:00, 11.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "결과 요약:\n",
      "Train Set: 81650 files (N: 40890, Y: 40760)\n",
      "Validation Set: 10200 files (N: 5110, Y: 5090)\n",
      "Test Set: 10230 files (N: 5120, Y: 5110)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# N 데이터 복사\n",
    "train_files_copied_N = copy_files(destination=train_folder, groups=train_groups_N, group_dict=file_groups_N, source=\"N 데이터\")\n",
    "val_files_copied_N = copy_files(destination=val_folder, groups=val_groups_N, group_dict=file_groups_N, source=\"N 데이터\")\n",
    "test_files_copied_N = copy_files(destination=test_folder, groups=test_groups_N, group_dict=file_groups_N, source=\"N 데이터\")\n",
    "\n",
    "# Y 데이터 복사\n",
    "train_files_copied_Y = copy_files(destination=train_folder, groups=train_groups_Y, group_dict=file_groups_Y, source=\"Y 데이터\")\n",
    "val_files_copied_Y = copy_files(destination=val_folder, groups=val_groups_Y, group_dict=file_groups_Y, source=\"Y 데이터\")\n",
    "test_files_copied_Y = copy_files(destination=test_folder, groups=test_groups_Y, group_dict=file_groups_Y, source=\"Y 데이터\")\n",
    "\n",
    "# 총 파일 개수 요약 출력\n",
    "total_train_files = train_files_copied_N + train_files_copied_Y\n",
    "total_val_files = val_files_copied_N + val_files_copied_Y\n",
    "total_test_files = test_files_copied_N + test_files_copied_Y\n",
    "\n",
    "print(f\"\\n결과 요약:\")\n",
    "print(f\"Train Set: {total_train_files} files (N: {train_files_copied_N}, Y: {train_files_copied_Y})\")\n",
    "print(f\"Validation Set: {total_val_files} files (N: {val_files_copied_N}, Y: {val_files_copied_Y})\")\n",
    "print(f\"Test Set: {total_test_files} files (N: {test_files_copied_N}, Y: {test_files_copied_Y})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train JSON files: 100%|██████████| 81650/81650 [01:21<00:00, 1005.19it/s]\n",
      "Processing Val JSON files: 100%|██████████| 10200/10200 [00:09<00:00, 1059.78it/s]\n",
      "Processing Test JSON files: 100%|██████████| 10230/10230 [00:09<00:00, 1074.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# Sorted_Dataset으로 옮겨온 Image와 동일한 이름의 JSON 파일을 label 폴더로 전부 복사\n",
    "def copy_json_files(base_dir):\n",
    "    folders = ['Train', 'Val', 'Test']\n",
    "    json_files = glob.glob('D:\\\\Falldown\\\\Dataset\\\\Original_Dataset\\\\*\\\\LabelingData\\\\Image\\\\**\\\\*.json', recursive=True)\n",
    "\n",
    "    if not json_files:\n",
    "        print(\"Warning: Source 경로에 JSON file이 없습니다.\")\n",
    "        return\n",
    "\n",
    "    # json_files를 {파일이름: 경로} 형식의 딕셔너리로 변환\n",
    "    json_dict = {os.path.splitext(os.path.basename(json_file))[0]: json_file for json_file in json_files}\n",
    "\n",
    "    skipped_json = 0  # 이미 존재하는 JSON 파일 수\n",
    "    copied_json = 0  # 새로 복사된 JSON 파일 수\n",
    "\n",
    "    for folder in folders:\n",
    "        images_dir = os.path.join(base_dir, folder, 'Image')\n",
    "        labels_dir = os.path.join(base_dir, folder, 'Label')\n",
    "\n",
    "        if not os.path.exists(labels_dir):\n",
    "            os.makedirs(labels_dir)\n",
    "\n",
    "        # Image 폴더의 모든 이미지 파일에 대해\n",
    "        image_files = os.listdir(images_dir)\n",
    "        if not image_files:\n",
    "            print(f\"Warning: {images_dir} 디렉토리에 이미지 파일이 없습니다.\")\n",
    "            continue\n",
    "\n",
    "        # 이미지 파일의 이름만 추출 (확장자 제외)\n",
    "        for image_file in tqdm(image_files, desc=f\"Processing {folder} JSON files\"):\n",
    "            image_name = os.path.splitext(image_file)[0]\n",
    "            # 동일한 이름의 JSON 파일이 딕셔너리에 존재하면 labels 폴더로 복사\n",
    "            if image_name in json_dict:\n",
    "                json_path = json_dict[image_name]\n",
    "                destination_path = os.path.join(labels_dir, f\"{image_name}.json\")\n",
    "\n",
    "                try:\n",
    "                    if os.path.exists(destination_path):  # 파일이 이미 존재하면 건너뜀\n",
    "                        skipped_json += 1\n",
    "                        continue\n",
    "                    shutil.copy(json_path, destination_path)\n",
    "                    copied_json += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {image_name}.json를 복사하는 중 문제가 발생했습니다. {e}\")\n",
    "            else:\n",
    "                print(f\"Warning: {image_file}에 해당되는 JSON 파일이 없습니다.\")\n",
    "\n",
    "    # 처리 결과 요약 출력\n",
    "    print(f\"총 JSON 파일 처리 완료: {copied_json}개 파일을 복사하고, {skipped_json}개 파일을 건너뛰었습니다.\")\n",
    "\n",
    "# 경로 지정\n",
    "base_directory = 'D:\\\\Falldown\\\\Dataset\\\\Sorted_Dataset'  # Image를 옮겨왔던 Sorted_Dataset 경로\n",
    "\n",
    "copy_json_files(base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
