{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "beb6bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.spatial import distance\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d59ceb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "468036e1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 랜드마크 인덱스 정의 # 11개\n",
    "LANDMARKS = [0, 11, 12, 15, 16, 23, 24, 25, 26, 27, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ec35a6a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 데이터 증강 함수 정의\n",
    "def augment_sequence(sequence, factor=0.2):\n",
    "    time_warped = []\n",
    "    for landmark in sequence:\n",
    "        x = np.arange(len(landmark))\n",
    "        f = interp1d(x, landmark, kind='linear', axis=0)\n",
    "        x_new = np.linspace(0, len(landmark) - 1, num=int(len(landmark) * (1 + factor)))\n",
    "        time_warped.append(f(x_new))\n",
    "    return np.array(time_warped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18debf6c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 바운딩 박스 비율 클래스 정의 함수\n",
    "def bbox_ratio_class(ratio):\n",
    "    if ratio < 0.7:\n",
    "        return 0  # Normal 가능성 높은 class\n",
    "    else:\n",
    "        return 1  # Danger or Fall 가능성 높은 class\n",
    "\n",
    "# YOLO xy 비율 계산 함수\n",
    "def calculate_yolo_xy_ratio(frame):\n",
    "    bbox = frame.get('bbox', None)\n",
    "    if bbox and (bbox['x2'] - bbox['x1']) != 0:\n",
    "        yolo_xy_ratio = round((bbox['y2'] - bbox['y1']) / (bbox['x2'] - bbox['x1']), 2)\n",
    "    else:\n",
    "        yolo_xy_ratio = 0.0\n",
    "    return yolo_xy_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea132622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머리 및 상체 속도 계산 함수 (시퀀스 평균값 사용)\n",
    "def calculate_head_upper_body_speed(sequence):\n",
    "    speeds = []\n",
    "    for j in range(1, len(sequence)):\n",
    "        keypoints = sequence[j]\n",
    "        prev_keypoints = sequence[j - 1]\n",
    "        h = np.array([keypoints.get(f'landmark_0', {}).get('x', 0.0), keypoints.get(f'landmark_0', {}).get('y', 0.0)])\n",
    "        l = np.array([keypoints.get(f'landmark_11', {}).get('x', 0.0), keypoints.get(f'landmark_11', {}).get('y', 0.0)])\n",
    "        r = np.array([keypoints.get(f'landmark_12', {}).get('x', 0.0), keypoints.get(f'landmark_12', {}).get('y', 0.0)])\n",
    "\n",
    "        # 이전 프레임의 좌표\n",
    "        prev_h = np.array([prev_keypoints.get(f'landmark_0', {}).get('x', 0.0), prev_keypoints.get(f'landmark_0', {}).get('y', 0.0)])\n",
    "        prev_l = np.array([prev_keypoints.get(f'landmark_11', {}).get('x', 0.0), prev_keypoints.get(f'landmark_11', {}).get('y', 0.0)])\n",
    "        prev_r = np.array([prev_keypoints.get(f'landmark_12', {}).get('x', 0.0), prev_keypoints.get(f'landmark_12', {}).get('y', 0.0)])\n",
    "\n",
    "        # 현재 프레임과 이전 프레임의 상체 중심\n",
    "        center_new = (h + l + r) / 3\n",
    "        center_prev = (prev_h + prev_l + prev_r) / 3\n",
    "\n",
    "        # 유클리드 거리 계산 (6프레임당 일정하므로 속력이라 봐도 무방함)\n",
    "        dist_new = distance.euclidean(center_new, center_prev)\n",
    "        speeds.append(dist_new)\n",
    "\n",
    "    # 평균 속력 계산\n",
    "    if speeds:\n",
    "        return sum(speeds) / len(speeds)\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "594bd98f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 데이터셋 클래스 정의\n",
    "# 현재 input은 mediapipe의 관절 좌표값 22개 + YOLO xy ratio + bbox_ratio_class + 머리/상체 속도 => 25개\n",
    "class FallSequenceDataset(Dataset):\n",
    "    def __init__(self, json_files, sequence_length=3, input_config='full'):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "        self.scaler = StandardScaler()\n",
    "        self.input_config = input_config\n",
    "\n",
    "        all_landmarks = []\n",
    "\n",
    "        for json_file in tqdm(json_files, desc=\"Processing JSON files\"):\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            frames = list(data['pose_data'].values())\n",
    "\n",
    "            for i in range(0, len(frames) - self.sequence_length + 1, self.sequence_length):\n",
    "                sequence = frames[i:i + self.sequence_length]\n",
    "                landmarks = []\n",
    "\n",
    "                for j, frame in enumerate(sequence):\n",
    "                    frame_landmarks = []\n",
    "                    if frame is not None:\n",
    "                        if self.input_config in ['full', 'mediapipe']:\n",
    "                            for landmark in LANDMARKS:\n",
    "                                landmark_data = frame.get(f'landmark_{landmark}', None)\n",
    "                                if landmark_data:\n",
    "                                    frame_landmarks.extend([\n",
    "                                        round(landmark_data['x'], 3),  # 소수점 세 자리로 반올림\n",
    "                                        round(landmark_data['y'], 3)\n",
    "                                    ])\n",
    "                                else:\n",
    "                                    frame_landmarks.extend([0.0, 0.0])\n",
    "\n",
    "                        if self.input_config in ['full', 'simplified']:\n",
    "                            # YOLO xy ratio 추가\n",
    "                            yolo_xy_ratio = calculate_yolo_xy_ratio(frame)\n",
    "                            frame_landmarks.append(yolo_xy_ratio)\n",
    "\n",
    "                            # 바운딩 박스 비율에 따른 클래스 추가 (bbox_ratio_class)\n",
    "                            ratio_class = bbox_ratio_class(yolo_xy_ratio)\n",
    "                            frame_landmarks.append(ratio_class)\n",
    "\n",
    "                        if self.input_config in ['full', 'simplified']:\n",
    "                            # 머리/상체 속도 추가 (시퀀스 평균 속력)\n",
    "                            head_torso_speed = calculate_head_upper_body_speed(sequence)\n",
    "                            frame_landmarks.append(head_torso_speed)\n",
    "\n",
    "                    else:\n",
    "                        if self.input_config == 'full':\n",
    "                            frame_landmarks.extend([0.0] * (len(LANDMARKS) * 2 + 3))  # 기본값으로 채우기 (YOLO xy 비율, bbox class, head/torso speed 포함)\n",
    "                        elif self.input_config == 'simplified':\n",
    "                            frame_landmarks.extend([0.0] * (3))\n",
    "                        elif self.input_config == 'mediapipe':\n",
    "                            frame_landmarks.extend([0.0] * (len(LANDMARKS) * 2))\n",
    "\n",
    "                    landmarks.append(frame_landmarks)\n",
    "\n",
    "                # 데이터 증강 적용\n",
    "                augmented_sequence = augment_sequence(landmarks)\n",
    "                all_landmarks.extend(augmented_sequence)\n",
    "\n",
    "                # 레이블 재정의\n",
    "                if sequence[-1]['class'] == 'Normal':\n",
    "                    label = 0  # 비낙상\n",
    "                elif sequence[-1]['class'] == 'Danger':\n",
    "                    label = 1  # 낙상 위험\n",
    "                elif sequence[-1]['class'] == 'Fall':\n",
    "                    label = 2  # 완전 낙상\n",
    "\n",
    "                self.sequences.append(augmented_sequence)\n",
    "                self.labels.append(label)\n",
    "\n",
    "        # 전체 데이터 정규화\n",
    "        all_landmarks = np.array(all_landmarks)\n",
    "        all_landmarks_scaled = self.scaler.fit_transform(all_landmarks)\n",
    "\n",
    "        # 정규화된 데이터를 다시 시퀀스로 재구성\n",
    "        for i in range(len(self.sequences)):\n",
    "            start = i * self.sequence_length\n",
    "            end = start + self.sequence_length\n",
    "            self.sequences[i] = all_landmarks_scaled[start:end]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.sequences[idx]), torch.LongTensor([self.labels[idx]]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ddba071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU 기반 낙상 감지 모델 정의\n",
    "class FallDetectionGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, num_classes=3):\n",
    "        super(FallDetectionGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(2, x.size(0), 128).to(x.device)  # 초기 은닉 상태 정의\n",
    "        out, _ = self.gru(x, h_0)\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 time step의 출력을 사용\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c562b8d5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files: 100%|██████████| 1935/1935 [00:21<00:00, 91.28it/s]\n",
      "Processing JSON files: 100%|██████████| 386/386 [00:04<00:00, 93.73it/s]\n",
      "Processing JSON files: 100%|██████████| 397/397 [00:04<00:00, 96.03it/s]\n",
      "Processing JSON files: 100%|██████████| 1935/1935 [00:17<00:00, 110.08it/s]\n",
      "Processing JSON files: 100%|██████████| 386/386 [00:03<00:00, 108.45it/s]\n",
      "Processing JSON files: 100%|██████████| 397/397 [00:03<00:00, 107.46it/s]\n",
      "Processing JSON files: 100%|██████████| 1935/1935 [00:14<00:00, 130.79it/s]\n",
      "Processing JSON files: 100%|██████████| 386/386 [00:02<00:00, 134.23it/s]\n",
      "Processing JSON files: 100%|██████████| 397/397 [00:03<00:00, 126.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# 검증 및 테스트 데이터셋 로드 경로\n",
    "train_json_folder = r'D:\\Falldown\\Dataset\\Video_Dataset\\Json_combined\\Train'\n",
    "valid_json_folder = r'D:\\Falldown\\Dataset\\Video_Dataset\\Json_combined\\Val'\n",
    "test_json_folder = r'D:\\Falldown\\Dataset\\Video_Dataset\\Json_combined\\Test'\n",
    "\n",
    "# 각 데이터 폴더에서 JSON 파일 목록 생성\n",
    "train_json_files = [os.path.join(train_json_folder, f) for f in os.listdir(train_json_folder) if f.endswith('.json')]\n",
    "valid_json_files = [os.path.join(valid_json_folder, f) for f in os.listdir(valid_json_folder) if f.endswith('.json')]\n",
    "test_json_files = [os.path.join(test_json_folder, f) for f in os.listdir(test_json_folder) if f.endswith('.json')]\n",
    "\n",
    "# 데이터셋 생성 (입력 데이터 설정에 따라)\n",
    "train_full_dataset = FallSequenceDataset(train_json_files, input_config='full')\n",
    "valid_full_dataset = FallSequenceDataset(valid_json_files, input_config='full')\n",
    "test_full_dataset = FallSequenceDataset(test_json_files, input_config='full')\n",
    "\n",
    "train_simplified_dataset = FallSequenceDataset(train_json_files, input_config='simplified')\n",
    "valid_simplified_dataset = FallSequenceDataset(valid_json_files, input_config='simplified')\n",
    "test_simplified_dataset = FallSequenceDataset(test_json_files, input_config='simplified')\n",
    "\n",
    "train_mediapipe_dataset = FallSequenceDataset(train_json_files, input_config='mediapipe')\n",
    "valid_mediapipe_dataset = FallSequenceDataset(valid_json_files, input_config='mediapipe')\n",
    "test_mediapipe_dataset = FallSequenceDataset(test_json_files, input_config='mediapipe')\n",
    "\n",
    "# 데이터 로더 생성\n",
    "train_loader_full = DataLoader(train_full_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader_full = DataLoader(valid_full_dataset, batch_size=32, shuffle=False)\n",
    "test_loader_full = DataLoader(test_full_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "train_loader_simplified = DataLoader(train_simplified_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader_simplified = DataLoader(valid_simplified_dataset, batch_size=32, shuffle=False)\n",
    "test_loader_simplified = DataLoader(test_simplified_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "train_loader_mediapipe = DataLoader(train_mediapipe_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader_mediapipe = DataLoader(valid_mediapipe_dataset, batch_size=32, shuffle=False)\n",
    "test_loader_mediapipe = DataLoader(test_mediapipe_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c2fe890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 가중치 계산 및 손실 함수 정의 (훈련 데이터셋 기준)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_full_dataset.labels), y=train_full_dataset.labels)\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# 모델 준비\n",
    "if len(train_full_dataset) > 0:\n",
    "    sample_sequence, sample_label = train_full_dataset[0]\n",
    "    input_size = sample_sequence.shape[1]\n",
    "    model = FallDetectionGRU(input_size).to(device)\n",
    "else:\n",
    "    print(\"데이터 없음\")\n",
    "    exit()\n",
    "\n",
    "# 옵티마이저와 스케줄러 설정\n",
    "optimizer_001 = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "optimizer_0001 = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "scheduler_001 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_001, mode='min', factor=0.1, patience=5)\n",
    "scheduler_0001 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_0001, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "# 학습 파라미터 설정\n",
    "num_epochs = 500\n",
    "best_loss = float('inf')\n",
    "patience = 50\n",
    "no_improve = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c39017e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Training Configuration: GRU with input_config='full' and lr='0.001' ====================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/500: 100%|██████████| 1972/1972 [00:06<00:00, 287.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 0.4376\n",
      "Epoch [1/500], Valid Loss: 0.4070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/500: 100%|██████████| 1972/1972 [00:06<00:00, 295.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/500], Train Loss: 0.3867\n",
      "Epoch [2/500], Valid Loss: 0.3965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/500: 100%|██████████| 1972/1972 [00:06<00:00, 301.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/500], Train Loss: 0.3651\n",
      "Epoch [3/500], Valid Loss: 0.3590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/500:  64%|██████▍   | 1270/1972 [00:03<00:01, 351.11it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m sequences, labels \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     18\u001b[0m loss_train\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\CHO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\CHO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[48], line 10\u001b[0m, in \u001b[0;36mFallDetectionGRU.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m      9\u001b[0m     h_0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m2\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m128\u001b[39m)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# 초기 은닉 상태 정의\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])  \u001b[38;5;66;03m# 마지막 time step의 출력을 사용\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\CHO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\CHO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\CHO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1392\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1392\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1393\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1395\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1396\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1397\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1398\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1399\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1400\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1401\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1404\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\n\u001b[0;32m   1405\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1406\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1413\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1414\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for optimizer, scheduler, train_loader, valid_loader, model_name in [\n",
    "    (optimizer_001, scheduler_001, train_loader_full, valid_loader_full, 'best_fall_detection_gru_001_full.pt'),\n",
    "    (optimizer_0001, scheduler_0001, train_loader_full, valid_loader_full, 'best_fall_detection_gru_0001_full.pt'),\n",
    "    (optimizer_0001, scheduler_0001, train_loader_simplified, valid_loader_simplified, 'best_fall_detection_gru_0001_simplified.pt'),\n",
    "    (optimizer_0001, scheduler_0001, train_loader_mediapipe, valid_loader_mediapipe, 'best_fall_detection_gru_0001_mediapipe.pt')\n",
    "]:\n",
    "    print(f\"{model_name} 학습 시작\")\n",
    "    print(\"\\n\" + \"=\" * 20 + f\" Training Configuration: GRU with input_config='{train_loader.dataset.input_config}' and lr='{optimizer.param_groups[0]['lr']}' \" + \"=\" * 20 + \"\\n\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for sequences, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss_train = criterion(outputs, labels.view(-1))\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss_train += loss_train.item()\n",
    "\n",
    "        avg_loss_train = total_loss_train / len(train_loader)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_loss_train:.4f}')\n",
    "\n",
    "        # 검증 데이터셋 평가\n",
    "        model.eval()\n",
    "        total_loss_valid = 0\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in valid_loader:\n",
    "                sequences, labels = sequences.to(device), labels.to(device)\n",
    "                outputs = model(sequences)\n",
    "                loss_valid = criterion(outputs, labels.view(-1))\n",
    "                total_loss_valid += loss_valid.item()\n",
    "\n",
    "        avg_loss_valid = total_loss_valid / len(valid_loader)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Valid Loss: {avg_loss_valid:.4f}')\n",
    "\n",
    "        # 학습률 조정 스케줄러 업데이트\n",
    "        scheduler.step(avg_loss_valid)\n",
    "\n",
    "        # 최상의 모델 저장\n",
    "        if avg_loss_valid < best_loss:\n",
    "            best_loss = avg_loss_valid\n",
    "            no_improve = 0\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        if no_improve >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2a4044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 지표 계산 함수 정의\n",
    "def calculate_metrics(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in data_loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            outputs = model(sequences)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return f1, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c64456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 완료된 후 검증 및 테스트 데이터셋에 대한 성능 평가\n",
    "for test_loader in [test_loader_full, test_loader_simplified, test_loader_mediapipe]:\n",
    "    test_f1, test_cm = calculate_metrics(model, test_loader)\n",
    "    print(f'Test F1: {test_f1:.4f}')\n",
    "    print(f'Test CM:\\n{test_cm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df25d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CNN 기반 낙상 감지 모델 정의\n",
    "# class FallDetectionCNN(nn.Module):\n",
    "#     def __init__(self, input_size, num_classes=3):\n",
    "#         super(FallDetectionCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "#         self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "#         self.fc1 = nn.Linear(128 * input_size, 256)\n",
    "#         self.fc2 = nn.Linear(256, 128)\n",
    "#         self.fc3 = nn.Linear(128, num_classes)\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.unsqueeze(1)  # Conv1d expects (batch_size, channels, length), adding channel dimension\n",
    "#         x = torch.relu(self.conv1(x))\n",
    "#         x = torch.relu(self.conv2(x))\n",
    "#         x = torch.relu(self.conv3(x))\n",
    "#         x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
