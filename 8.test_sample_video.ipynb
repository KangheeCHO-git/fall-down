{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f1198e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import mediapipe as mp\n",
    "import warnings\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import softmax\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b974a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv8s model 호출\n",
    "yolo_model = YOLO('D:\\\\Falldown\\\\code-git\\\\runs\\\\detect\\\\human_fall_s\\\\weights\\\\best.pt')\n",
    "\n",
    "# MediaPipe Pose 호출\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.3) # 학습 시 데이터와 동일한 임계값 설정\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# 랜드마크 인덱스 정의 # 11개\n",
    "LANDMARKS = [0, 11, 12, 15, 16, 23, 24, 25, 26, 27, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f1168ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU 기반 낙상 감지 모델 정의\n",
    "class FallDetectionGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, num_classes=3):\n",
    "        super(FallDetectionGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(2, x.size(0), 128).to(x.device)  # 초기 은닉 상태 정의\n",
    "        out, _ = self.gru(x, h_0)\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 time step의 출력을 사용\n",
    "        return out\n",
    "\n",
    "def load_gru_model(model_path):\n",
    "    # 모델을 생성할 때 올바른 input_size를 사용\n",
    "    if \"full\" in model_path:\n",
    "        input_size = 25\n",
    "    elif \"simplified\" in model_path:\n",
    "        input_size = 3\n",
    "    elif \"mediapipe\" in model_path:\n",
    "        input_size = 22\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model type in filename. Please check the model name.\")\n",
    "\n",
    "    # FallDetectionGRU 모델 생성 및 가중치 로드\n",
    "    gru_model = FallDetectionGRU(input_size=input_size, hidden_size=128, num_layers=2, num_classes=3)\n",
    "    gru_model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "    gru_model.eval()\n",
    "\n",
    "    return gru_model, input_size\n",
    "\n",
    "def resize_and_pad_frame(frame, target_size=(640, 640)):\n",
    "    h, w = frame.shape[:2]\n",
    "    scale = min(target_size[0] / w, target_size[1] / h)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    resized_frame = cv2.resize(frame, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "    pad_w, pad_h = (target_size[0] - new_w) // 2, (target_size[1] - new_h) // 2\n",
    "    padded_frame = cv2.copyMakeBorder(resized_frame, pad_h, target_size[1] - new_h - pad_h,\n",
    "                                      pad_w, target_size[0] - new_w - pad_w, cv2.BORDER_CONSTANT, value=[128, 128, 128])\n",
    "    return padded_frame, new_w, new_h, pad_w, pad_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5794be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_yolo_xy_ratio(bbox):\n",
    "    if len(bbox) > 0 and (bbox[2] - bbox[0]) != 0:\n",
    "        return round((bbox[3] - bbox[1]) / (bbox[2] - bbox[0]), 3)\n",
    "    return 0.0\n",
    "\n",
    "def bbox_ratio_class(ratio):\n",
    "    return 0 if ratio < 0.7 else 1 # 0.7 이상은 Normal일 확률이 높음\n",
    "\n",
    "# 상체 속력 계산\n",
    "def calculate_head_upper_body_speed(sequence):\n",
    "    speeds = []\n",
    "    for j in range(1, len(sequence)):\n",
    "        keypoints = sequence[j]\n",
    "        prev_keypoints = sequence[j - 1]\n",
    "        h = np.array([keypoints[0][0], keypoints[0][1]])  # 머리\n",
    "        l = np.array([keypoints[11][0], keypoints[11][1]])  # 좌측 어깨\n",
    "        r = np.array([keypoints[12][0], keypoints[12][1]])  # 우측 어깨\n",
    "\n",
    "        prev_h = np.array([prev_keypoints[0][0], prev_keypoints[0][1]])\n",
    "        prev_l = np.array([prev_keypoints[11][0], prev_keypoints[11][1]])\n",
    "        prev_r = np.array([prev_keypoints[12][0], prev_keypoints[12][1]])\n",
    "\n",
    "        center_new = (h + l + r) / 3\n",
    "        center_prev = (prev_h + prev_l + prev_r) / 3\n",
    "\n",
    "        speeds.append(distance.euclidean(center_new, center_prev))\n",
    "\n",
    "    return sum(speeds) / len(speeds) if speeds else 0.0\n",
    "\n",
    "# 추가적인 특징을 계산하는 함수\n",
    "def calculate_additional_features(bbox, joint_sequence):\n",
    "    yolo_xy_ratio = calculate_yolo_xy_ratio(bbox)\n",
    "    ratio_class = bbox_ratio_class(yolo_xy_ratio)\n",
    "    speed = calculate_head_upper_body_speed(joint_sequence)\n",
    "    return yolo_xy_ratio, ratio_class, speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7a51459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 바운딩 박스 크기 조절(20% 확대)\n",
    "def adjust_bbox(bbox, scale_factor, frame_shape):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    center_x = (x1 + x2) / 2\n",
    "    center_y = (y1 + y2) / 2\n",
    "    \n",
    "    new_width = width * scale_factor\n",
    "    new_height = height * scale_factor\n",
    "    \n",
    "    new_x1 = max(0, int(center_x - new_width / 2))\n",
    "    new_y1 = max(0, int(center_y - new_height / 2))\n",
    "    new_x2 = min(int(center_x + new_width / 2), frame_shape[1])  # 우측 하단 x 좌표를 조정, 이미지 폭(frame_shape[1]) 초과 방지\n",
    "    new_y2 = min(int(center_y + new_height / 2), frame_shape[0])  # 우측 하단 y 좌표를 조정, 이미지 높이(frame_shape[0]) 초과 방지\n",
    "    \n",
    "    return [new_x1, new_y1, new_x2, new_y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283238e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비디오 프레임을 처리하는 함수\n",
    "def process_frame(frame, yolo_model, pose, target_size):\n",
    "    joint_sequence = []\n",
    "    resized_frame, new_width, new_height, pad_w, pad_h = resize_and_pad_frame(frame, target_size)\n",
    "    try:\n",
    "        results = yolo_model(resized_frame, verbose=False)\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"Error during YOLO inference: {e}\")\n",
    "        return frame, joint_sequence, None\n",
    "\n",
    "    # Yolo 시행 후 bbox 도출\n",
    "    bbox = []\n",
    "    # 바운딩 박스 추출 및 크기 조정\n",
    "    for result in results[0].boxes:\n",
    "        x1, y1, x2, y2 = map(int, result.xyxy[0])\n",
    "        adjusted_bbox = adjust_bbox([x1, y1, x2, y2], scale_factor=1.2, frame_shape=resized_frame.shape)\n",
    "        bbox.append((adjusted_bbox[0], adjusted_bbox[1], adjusted_bbox[2], adjusted_bbox[3]))\n",
    "\n",
    "    # Mediapipe 시행 후 joint_sequence 도출\n",
    "    joint_coords = []\n",
    "    if len(bbox) > 0:\n",
    "        person_image = resized_frame[bbox[0][1]:bbox[0][3], bbox[0][0]:bbox[0][2]]\n",
    "        person_image_rgb = cv2.cvtColor(person_image, cv2.COLOR_BGR2RGB)\n",
    "        results_pose = pose.process(person_image_rgb)\n",
    "\n",
    "        if results_pose.pose_landmarks:\n",
    "            for landmark in results_pose.pose_landmarks.landmark:\n",
    "                # 조정된 바운딩 박스 내의 좌표를 전체 프레임의 좌표로 변환\n",
    "                global_x = bbox[0][0] + landmark.x * (bbox[0][2] - bbox[0][0])\n",
    "                global_y = bbox[0][1] + landmark.y * (bbox[0][3] - bbox[0][1])\n",
    "                joint_coords.append((global_x, global_y))\n",
    "            joint_sequence.append(joint_coords)\n",
    "\n",
    "    return results_pose, bbox, joint_coords, joint_sequence, new_width, new_height, pad_w, pad_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6a0e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU 모델에 input_features를 생성하는 함수\n",
    "def generate_input_features(bbox, joint_coords, joint_sequence, input_size):\n",
    "    input_features = []\n",
    "\n",
    "    if input_size == 25 and len(joint_coords) >= 11 and len(bbox) > 0:\n",
    "        joint_array = []\n",
    "        for landmark_idx in LANDMARKS:\n",
    "            if landmark_idx < len(joint_coords):\n",
    "                x, y = joint_coords[landmark_idx]\n",
    "                joint_array.extend([x, y])\n",
    "            else:\n",
    "                joint_array.extend([0.0, 0.0])\n",
    "\n",
    "        yolo_xy_ratio, ratio_class, speed = calculate_additional_features(bbox[0], joint_sequence)\n",
    "        input_features = np.array(joint_array)\n",
    "        input_features = np.concatenate((input_features, [yolo_xy_ratio, ratio_class, speed]))\n",
    "\n",
    "        if len(input_features) != 25:\n",
    "            raise ValueError(f\"Expected input_features length to be 25, but got {len(input_features)}. Please check the feature extraction.\")\n",
    "\n",
    "    elif input_size == 22 and len(joint_coords) >= 11:\n",
    "        joint_array = []\n",
    "        for landmark_idx in LANDMARKS:\n",
    "            if landmark_idx < len(joint_coords):\n",
    "                x, y = joint_coords[landmark_idx]\n",
    "                joint_array.extend([x, y])\n",
    "            else:\n",
    "                joint_array.extend([0.0, 0.0])\n",
    "        input_features = np.array(joint_array)\n",
    "\n",
    "        if len(input_features) != 22:\n",
    "            raise ValueError(f\"Expected input_features length to be 22, but got {len(input_features)}. Please check the joint coordinates.\")\n",
    "\n",
    "    elif input_size == 3 and len(bbox) > 0:\n",
    "        yolo_xy_ratio, ratio_class, speed = calculate_additional_features(bbox[0], joint_sequence)\n",
    "        input_features = np.array([yolo_xy_ratio, ratio_class, speed])\n",
    "\n",
    "        if len(input_features) != 3:\n",
    "            raise ValueError(f\"Expected input_features length to be 3, but got {len(input_features)}. Please check the bbox features.\")\n",
    "\n",
    "    else:\n",
    "        input_features = None\n",
    "\n",
    "    return input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87e0e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 바운딩 박스와 포즈 결과를 시각화하는 함수\n",
    "def visualize_results(frame, bbox, previous_pred_class, results_pose, new_width, new_height, pad_w, pad_h, original_width, original_height):\n",
    "    color_map = {\n",
    "        0: (0, 255, 0),      # Normal: 초록색\n",
    "        1: (0, 165, 255),    # Danger: 주황색\n",
    "        2: (0, 0, 255)       # Fall: 빨간색\n",
    "    }\n",
    "    label_map = {\n",
    "        0: 'Normal',    # 비낙상\n",
    "        1: 'Danger',    # 낙상 위험\n",
    "        2: 'Fall'       # 완전 낙상\n",
    "    }\n",
    "    color = color_map.get(previous_pred_class, (255, 255, 255))  # 기본값은 흰색\n",
    "    label_text = label_map.get(previous_pred_class, 'Unknown')   # 기본값은 'Unknown'\n",
    "\n",
    "    # Yolo bbox 시각화\n",
    "    for adjusted_bbox in bbox:\n",
    "        x1, y1, x2, y2 = adjusted_bbox\n",
    "        original_x1 = (x1 - pad_w) * (original_width / new_width)\n",
    "        original_y1 = (y1 - pad_h) * (original_height / new_height)\n",
    "        original_x2 = (x2 - pad_w) * (original_width / new_width)\n",
    "        original_y2 = (y2 - pad_h) * (original_height / new_height)\n",
    "\n",
    "        label = f\"Class: {label_text}\"\n",
    "        cv2.rectangle(frame, (int(original_x1), int(original_y1)), (int(original_x2), int(original_y2)), color, thickness=3)\n",
    "        cv2.putText(frame, label, (int(original_x1), int(original_y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, thickness=3)\n",
    "\n",
    "    # Mediapipe 관절 좌표 시각화\n",
    "    if results_pose and results_pose.pose_landmarks:\n",
    "        for landmark in results_pose.pose_landmarks.landmark:\n",
    "            global_x = adjusted_bbox[0] + landmark.x * (adjusted_bbox[2] - adjusted_bbox[0])\n",
    "            global_y = adjusted_bbox[1] + landmark.y * (adjusted_bbox[3] - adjusted_bbox[1])\n",
    "\n",
    "            original_landmark_x = (global_x - pad_w) * (original_width / new_width)\n",
    "            original_landmark_y = (global_y - pad_h) * (original_height / new_height)\n",
    "\n",
    "            cv2.circle(frame, (int(original_landmark_x), int(original_landmark_y)), radius=3, color=(0, 255, 255), thickness=-1)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26211ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비디오를 처리하는 주 함수\n",
    "def process_video(video_path, output_path, model_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    target_size = (640, 640)\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (original_width, original_height)) # 원본 해상도로 저장하기 위해 VideoWriter를 원본 크기로 설정\n",
    "    gru_model, input_size = load_gru_model(model_path)\n",
    "\n",
    "    frame_idx = 0\n",
    "    joint_sequence = []  # 속도 계산을 위함\n",
    "    previous_pred_class = -1  # 이전 프레임의 예측 클래스\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            if frame_idx >= total_frames: # EOF에 도달한 경우\n",
    "                tqdm.write(\"End of video stream. Processing completed.\")  # tqdm의 write 사용\n",
    "                break\n",
    "            else: # 프레임 누락 오류\n",
    "                tqdm.write(f\"Warning: Failed to read frame {frame_idx}. Skipping to the next frame.\")\n",
    "                frame_idx += 1  # 누락된 프레임을 건너뛰기 위해 증가\n",
    "                continue\n",
    "\n",
    "        # 매 6번째 프레임마다 처리\n",
    "        if frame_idx % 6 == 0:\n",
    "            results_pose, bbox, joint_coords, joint_sequence, new_width, new_height, pad_w, pad_h = process_frame(frame, yolo_model, pose, target_size) # input 값 생성 및 모델 계산은 640x640 기준\n",
    "            if joint_sequence:\n",
    "                if bbox:\n",
    "                    input_features = generate_input_features(bbox, joint_coords, joint_sequence, input_size)\n",
    "                    if input_features is not None:\n",
    "                        # 입력 텐서의 형태: [input_size] -> [1, 1, input_size]로 변환\n",
    "                        input_tensor = torch.tensor(input_features, dtype=torch.float32).unsqueeze(0).unsqueeze(0) # [1, 1, input_size]\n",
    "                        output = gru_model(input_tensor)\n",
    "                        pred_class = torch.argmax(softmax(output, dim=1), dim=1).item()\n",
    "                        previous_pred_class = pred_class # 이전 예측 클래스 업데이트\n",
    "                else:\n",
    "                    pred_class = previous_pred_class # 감지되지 않는 경우 이전 예측 클래스를 유지\n",
    "\n",
    "        # 바운딩 박스와 포즈 시각화\n",
    "        frame = visualize_results(frame, bbox, previous_pred_class, results_pose, new_width, new_height, pad_w, pad_h, original_width, original_height)\n",
    "        out.write(frame)\n",
    "        \n",
    "        if frame_idx % 120 == 0: # 120 프레임마다 진행 상태 출력\n",
    "            tqdm.write(f\"Processing {os.path.basename(video_path)}: Frame {frame_idx}/{total_frames}\")\n",
    "        frame_idx += 1\n",
    "        \n",
    "    cap.release()\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "499f90e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4: Frame 0/600\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     output_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     23\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_directory, output_filename)\n\u001b[1;32m---> 25\u001b[0m     \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 비디오 처리 함수 호출\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     tqdm\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 처리 완료!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[17], line 31\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(video_path, output_path, model_path)\u001b[0m\n\u001b[0;32m     29\u001b[0m processed_frame, new_width, new_height, pad_w, pad_h \u001b[38;5;241m=\u001b[39m resize_and_pad_frame(frame, target_size) \u001b[38;5;66;03m# input 값 생성 및 모델 계산은 640x640 기준\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43myolo_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# YOLO 먼저 시행\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     33\u001b[0m     tqdm\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during YOLO inference: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\CHO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\model.py:176\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    149\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    150\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\CHO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\model.py:554\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\CHO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\CHO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\CHO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\predictor.py:253\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    250\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(im0s)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m    254\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n",
      "File \u001b[1;32mc:\\Users\\CHO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\utils\\ops.py:51\u001b[0m, in \u001b[0;36mProfile.__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m, value, traceback):  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Stop timing.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart  \u001b[38;5;66;03m# delta-time\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt\n",
      "File \u001b[1;32mc:\\Users\\CHO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\utils\\ops.py:61\u001b[0m, in \u001b[0;36mProfile.time\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get current time.\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda:\n\u001b[1;32m---> 61\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32mc:\\Users\\CHO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:954\u001b[0m, in \u001b[0;36msynchronize\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    952\u001b[0m _lazy_init()\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[1;32m--> 954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_synchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_group = ['best_fall_detection_gru_001_full.pt', 'best_fall_detection_gru_001_mediapipe.pt',\n",
    "               'best_fall_detection_gru_0001_simplified.pt'] # , 'best_fall_detection_cnn_mediapipe_0001.pt']\n",
    "\n",
    "video_group = ['D:\\\\Falldown\\\\Dataset\\\\Video_Dataset\\\\Video\\Test\\\\00028_H_A_FY_C1.mp4', #1\n",
    "               'D:\\\\Falldown\\\\Dataset\\\\Video_Dataset\\\\Video\\Test\\\\00130_H_A_FY_C2.mp4', #2\n",
    "               'D:\\\\Falldown\\\\Dataset\\\\Video_Dataset\\\\Video\\Test\\\\00712_H_D_BY_C3.mp4', #3\n",
    "               'D:\\\\Falldown\\\\Dataset\\\\Video_Dataset\\\\Video\\Test\\\\02900_Y_C_BY_C4.mp4', #4\n",
    "               'D:\\\\Falldown\\\\Dataset\\\\Video_Dataset\\\\Video\\Test\\\\02087_H_A_SY_C5.mp4', #5\n",
    "               'D:\\\\Falldown\\\\Dataset\\\\Video_Dataset\\\\Video\\Test\\\\01757_Y_E_SY_C6.mp4', #6\n",
    "               'D:\\\\Falldown\\\\Dataset\\\\Video_Dataset\\\\Video\\Test\\\\00690_H_D_N_C7.mp4', #7\n",
    "               'D:\\\\Falldown\\\\Dataset\\\\Video_Dataset\\\\Video\\Test\\\\00799_O_E_N_C8.mp4'  #8\n",
    "]\n",
    "\n",
    "output_directory = 'D:\\\\Falldown\\\\code-git'\n",
    "\n",
    "# Video 샘플 처리\n",
    "for model_path in model_group:\n",
    "    for video_path in video_group:\n",
    "        try:\n",
    "            suffix = f\"_{model_path.split('_')[3]}_{model_path.split('_')[5].replace('.pt', '')}.mp4\" if '_gru_' in model_path else f\"_{model_path.split('_')[3]}.mp4\"\n",
    "            video_name, _ = os.path.splitext(os.path.basename(video_path)) # 비디오 파일명 추출\n",
    "            output_filename = f\"{video_name}{suffix}\"\n",
    "            output_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "            process_video(video_path, output_path, model_path)  # 비디오 처리 함수 호출\n",
    "            tqdm.write(f\"✅ {video_name} 처리 완료!\")\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"❌ {video_name} 처리 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ed412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
