{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f1198e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from ultralytics import YOLO\n",
    "from scipy.spatial import distance\n",
    "from torch.nn.functional import softmax\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b974a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv8s model 호출\n",
    "yolo_model = YOLO('D:\\\\Falldown\\\\code-git\\\\runs\\\\detect\\\\human_fall_s30\\\\weights\\\\best.pt')\n",
    "\n",
    "# MediaPipe Pose 호출\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, enable_segmentation=False)\n",
    "\n",
    "# 랜드마크 인덱스 정의 # 11개\n",
    "LANDMARKS = [0, 11, 12, 15, 16, 23, 24, 25, 26, 27, 28]\n",
    "\n",
    "# GRU 기반 낙상 감지 모델 정의\n",
    "class FallDetectionGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, num_classes=3):\n",
    "        super(FallDetectionGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(2, x.size(0), 128).to(x.device)  # 초기 은닉 상태 정의\n",
    "        out, _ = self.gru(x, h_0)\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 time step의 출력을 사용\n",
    "        return out\n",
    "\n",
    "def load_gru_model(model_path):\n",
    "    # 모델을 생성할 때 올바른 input_size를 사용\n",
    "    if \"full\" in model_path:\n",
    "        input_size = 25\n",
    "    elif \"simplified\" in model_path:\n",
    "        input_size = 3\n",
    "    elif \"mediapipe\" in model_path:\n",
    "        input_size = 22\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model type in filename. Please check the model name.\")\n",
    "\n",
    "    # FallDetectionGRU 모델 생성 및 가중치 로드\n",
    "    gru_model = FallDetectionGRU(input_size=input_size, hidden_size=128, num_layers=2, num_classes=3)\n",
    "    gru_model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "    gru_model.eval()\n",
    "\n",
    "    return gru_model, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5794be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_ratio_class(ratio):\n",
    "    return 0 if ratio < 0.7 else 1 # 0.7 이상은 Normal일 확률이 높음\n",
    "\n",
    "def calculate_yolo_xy_ratio(bbox):\n",
    "    # bbox가 비어 있지 않고, 폭(width)이 0이 아닌 경우에만 계산\n",
    "    if len(bbox) > 0 and (bbox[2] - bbox[0]) != 0:\n",
    "        return round((bbox[3] - bbox[1]) / (bbox[2] - bbox[0]), 3)\n",
    "    return 0.0\n",
    "\n",
    "# Head and upper body speed calculation\n",
    "def calculate_head_upper_body_speed(sequence):\n",
    "    speeds = []\n",
    "    for j in range(1, len(sequence)):\n",
    "        keypoints = sequence[j]\n",
    "        prev_keypoints = sequence[j - 1]\n",
    "        h = np.array([keypoints[0][0], keypoints[0][1]])  # Head\n",
    "        l = np.array([keypoints[11][0], keypoints[11][1]])  # Left shoulder\n",
    "        r = np.array([keypoints[12][0], keypoints[12][1]])  # Right shoulder\n",
    "\n",
    "        prev_h = np.array([prev_keypoints[0][0], prev_keypoints[0][1]])\n",
    "        prev_l = np.array([prev_keypoints[11][0], prev_keypoints[11][1]])\n",
    "        prev_r = np.array([prev_keypoints[12][0], prev_keypoints[12][1]])\n",
    "\n",
    "        center_new = (h + l + r) / 3\n",
    "        center_prev = (prev_h + prev_l + prev_r) / 3\n",
    "\n",
    "        speeds.append(distance.euclidean(center_new, center_prev))\n",
    "\n",
    "    return sum(speeds) / len(speeds) if speeds else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7a51459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 바운딩 박스 크기 조절(20% 확대)\n",
    "def adjust_bbox(bbox, scale_factor, frame_shape):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    center_x = (x1 + x2) / 2\n",
    "    center_y = (y1 + y2) / 2\n",
    "    \n",
    "    new_width = width * scale_factor\n",
    "    new_height = height * scale_factor\n",
    "    \n",
    "    new_x1 = max(0, int(center_x - new_width / 2))\n",
    "    new_y1 = max(0, int(center_y - new_height / 2))\n",
    "    new_x2 = min(int(center_x + new_width / 2), frame_shape[1])  # 우측 하단 x 좌표를 조정, 이미지 폭(frame_shape[1]) 초과 방지\n",
    "    new_y2 = min(int(center_y + new_height / 2), frame_shape[0])  # 우측 하단 y 좌표를 조정, 이미지 높이(frame_shape[0]) 초과 방지\n",
    "    \n",
    "    return [new_x1, new_y1, new_x2, new_y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "26211ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 Video를 분석하는 함수\n",
    "def process_video(video_path, output_path, model_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "    gru_model, input_size = load_gru_model(model_path)\n",
    "\n",
    "    frame_idx = 0\n",
    "    joint_sequence = []  # 속도 계산을 위함\n",
    "    previous_pred_class = -1  # 이전 프레임의 예측 클래스\n",
    "\n",
    "    # tqdm 초기화\n",
    "    pbar = tqdm(total=total_frames, desc=f\"Processing {os.path.basename(video_path)}\", unit=\"frame\", leave=True)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_idx % 6 == 0:\n",
    "            # YOLO inference\n",
    "            results = yolo_model(frame)\n",
    "            bbox = []\n",
    "\n",
    "            # 바운딩 박스 추출 및 크기 조정\n",
    "            for result in results[0].boxes:\n",
    "                x1, y1, x2, y2 = map(int, result.xyxy[0])\n",
    "                adjusted_bbox = adjust_bbox([x1, y1, x2, y2], scale_factor=1.2, frame_shape=frame.shape)\n",
    "                bbox.append((adjusted_bbox[0], adjusted_bbox[1], adjusted_bbox[2], adjusted_bbox[3]))\n",
    "\n",
    "            # MediaPipe inference\n",
    "            joint_coords = []\n",
    "            if len(bbox) > 0:\n",
    "                # 조정된 바운딩 박스 사용\n",
    "                person_image = frame[bbox[0][1]:bbox[0][3], bbox[0][0]:bbox[0][2]]\n",
    "                person_image_rgb = cv2.cvtColor(person_image, cv2.COLOR_BGR2RGB)\n",
    "                results_pose = pose.process(person_image_rgb)\n",
    "\n",
    "                if results_pose.pose_landmarks:\n",
    "                    for landmark in results_pose.pose_landmarks.landmark:\n",
    "                        # 조정된 바운딩 박스 내의 좌표를 전체 프레임의 좌표로 변환\n",
    "                        global_x = (bbox[0][0] + landmark.x * person_image.shape[1]) / width\n",
    "                        global_y = (bbox[0][1] + landmark.y * person_image.shape[0]) / height\n",
    "                        joint_coords.append((global_x * width, global_y * height))\n",
    "                    joint_sequence.append(joint_coords)\n",
    "\n",
    "            # Prepare input for GRU\n",
    "            input_features = []\n",
    "\n",
    "            if input_size == 25 and len(joint_coords) >= 11 and len(bbox) > 0:\n",
    "                # 11개의 랜드마크 정보 추가 (각각 x, y 좌표를 사용)\n",
    "                joint_array = []\n",
    "                for landmark_idx in LANDMARKS:\n",
    "                    if landmark_idx < len(joint_coords):\n",
    "                        x, y = joint_coords[landmark_idx]\n",
    "                        joint_array.extend([x, y])  # 각 랜드마크의 x, y 좌표 추가\n",
    "                    else:\n",
    "                        # 랜드마크가 존재하지 않는 경우, (0, 0)을 추가\n",
    "                        joint_array.extend([0.0, 0.0])\n",
    "\n",
    "                # 추가적인 특징 추가\n",
    "                yolo_xy_ratio = calculate_yolo_xy_ratio(bbox[0])  # 바운딩 박스의 정보로부터 계산\n",
    "                ratio_class = bbox_ratio_class(yolo_xy_ratio)\n",
    "                speed = calculate_head_upper_body_speed(joint_sequence)\n",
    "\n",
    "                # 모든 특징들을 하나의 배열로 결합 (총 25개가 되어야 함)\n",
    "                input_features = np.array(joint_array)  # 총 22개의 관절 좌표 정보\n",
    "                input_features = np.concatenate((input_features, [yolo_xy_ratio, ratio_class, speed]))  # 22 + 3 = 25개의 특징\n",
    "\n",
    "                # 길이 확인\n",
    "                if len(input_features) != 25:\n",
    "                    raise ValueError(f\"Expected input_features length to be 25, but got {len(input_features)}. Please check the feature extraction.\")\n",
    "\n",
    "            elif input_size == 22 and len(joint_coords) >= 11:\n",
    "                # 11개의 랜드마크 좌표만 사용하는 경우 (각각 x, y 좌표)\n",
    "                joint_array = []\n",
    "                for landmark_idx in LANDMARKS:\n",
    "                    if landmark_idx < len(joint_coords):\n",
    "                        x, y = joint_coords[landmark_idx]\n",
    "                        joint_array.extend([x, y])  # 각 랜드마크의 x, y 좌표 추가\n",
    "                    else:\n",
    "                        # 랜드마크가 누락된 경우, (0, 0) 추가\n",
    "                        joint_array.extend([0.0, 0.0])\n",
    "                input_features = np.array(joint_array)\n",
    "\n",
    "                # 길이 확인\n",
    "                if len(input_features) != 22:\n",
    "                    raise ValueError(f\"Expected input_features length to be 22, but got {len(input_features)}. Please check the joint coordinates.\")\n",
    "\n",
    "            elif input_size == 3 and len(bbox) > 0:\n",
    "                # 간소화된 바운딩 박스 정보만 사용하는 경우\n",
    "                yolo_xy_ratio = calculate_yolo_xy_ratio(bbox[0])\n",
    "                ratio_class = bbox_ratio_class(yolo_xy_ratio)\n",
    "                speed = calculate_head_upper_body_speed(joint_sequence)\n",
    "\n",
    "                # 3개의 특징으로 구성\n",
    "                input_features = np.array([yolo_xy_ratio, ratio_class, speed])\n",
    "\n",
    "                # 길이 확인\n",
    "                if len(input_features) != 3:\n",
    "                    raise ValueError(f\"Expected input_features length to be 3, but got {len(input_features)}. Please check the bbox features.\")\n",
    "\n",
    "            else:\n",
    "                pred_class = previous_pred_class  # 감지되지 않는 경우 이전 예측 클래스를 유지\n",
    "                input_features = None\n",
    "\n",
    "            if input_features is not None:\n",
    "                # 입력 텐서의 형태: [input_size] -> [1, 1, input_size]로 변환\n",
    "                input_tensor = torch.tensor(input_features, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # [1, 1, input_size]\n",
    "\n",
    "                # GRU 모델에 전달 (sequence_length=1, batch_size=1, input_size=25)\n",
    "                output = gru_model(input_tensor)\n",
    "                pred_class = torch.argmax(softmax(output, dim=1), dim=1).item()\n",
    "                previous_pred_class = pred_class  # 이전 예측 클래스 업데이트\n",
    "            else:\n",
    "                pred_class = previous_pred_class\n",
    "\n",
    "        # Visualization (클래스에 따른 색상 설정)\n",
    "        color_map = {\n",
    "            0: (0, 255, 0),      # Normal: 초록색\n",
    "            1: (0, 165, 255),    # Danger: 주황색\n",
    "            2: (0, 0, 255)       # Fall: 빨간색\n",
    "        }\n",
    "        label_map = {\n",
    "            0: 'Normal',    # 비낙상\n",
    "            1: 'Danger',    # 낙상 위험\n",
    "            2: 'Fall'       # 완전 낙상\n",
    "        }\n",
    "        color = color_map.get(previous_pred_class, (255, 255, 255))  # 기본값은 흰색 (클래스가 없는 경우)\n",
    "        label_text = label_map.get(previous_pred_class, 'Unknown')   # 기본값은 'Unknown' (클래스가 없는 경우)\n",
    "\n",
    "        # 바운딩 박스 그리기\n",
    "        for box in bbox:\n",
    "            x1, y1, x2, y2 = box\n",
    "            label = f\"Class: {label_text}\"  # 클래스 이름을 직접 출력\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness=3)  # 바운딩 박스 테두리 두께를 3으로 설정\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, thickness=3)  # 글자 크기 및 두께 설정\n",
    "\n",
    "        out.write(frame) # frame 저장\n",
    "        pbar.update(1) # tqdm 업데이트\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "499f90e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:   0%|          | 0/600 [00:00<?, ?frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:   1%|          | 4/600 [00:00<01:12,  8.18frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:   2%|▏         | 10/600 [00:00<00:36, 16.17frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:   3%|▎         | 16/600 [00:01<00:29, 19.64frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 10.0ms\n",
      "Speed: 3.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:   4%|▎         | 22/600 [00:01<00:28, 20.43frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:   5%|▍         | 28/600 [00:01<00:26, 21.84frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:   6%|▌         | 34/600 [00:01<00:24, 22.87frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:   7%|▋         | 40/600 [00:02<00:25, 21.72frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:   8%|▊         | 46/600 [00:02<00:24, 22.46frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 29.0ms\n",
      "Speed: 2.0ms preprocess, 29.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:   9%|▊         | 52/600 [00:02<00:24, 22.20frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 37.0ms\n",
      "Speed: 3.0ms preprocess, 37.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  10%|▉         | 58/600 [00:03<00:24, 21.73frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 31.0ms\n",
      "Speed: 2.0ms preprocess, 31.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  11%|█         | 64/600 [00:03<00:25, 21.15frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 36.0ms\n",
      "Speed: 2.0ms preprocess, 36.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  12%|█▏        | 70/600 [00:03<00:24, 21.67frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 37.0ms\n",
      "Speed: 2.0ms preprocess, 37.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  13%|█▎        | 76/600 [00:03<00:24, 21.45frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 46.0ms\n",
      "Speed: 2.0ms preprocess, 46.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  14%|█▎        | 82/600 [00:04<00:25, 19.94frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 42.0ms\n",
      "Speed: 4.0ms preprocess, 42.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  15%|█▍        | 88/600 [00:04<00:25, 20.19frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 37.0ms\n",
      "Speed: 2.0ms preprocess, 37.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  16%|█▌        | 94/600 [00:04<00:24, 20.56frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 86.0ms\n",
      "Speed: 6.0ms preprocess, 86.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  17%|█▋        | 102/600 [00:05<00:25, 19.57frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 77.0ms\n",
      "Speed: 2.0ms preprocess, 77.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  18%|█▊        | 108/600 [00:05<00:25, 19.34frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 108.0ms\n",
      "Speed: 3.0ms preprocess, 108.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  19%|█▉        | 114/600 [00:06<00:26, 18.21frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 35.0ms\n",
      "Speed: 2.0ms preprocess, 35.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  20%|██        | 120/600 [00:06<00:25, 18.63frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 32.0ms\n",
      "Speed: 3.0ms preprocess, 32.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  21%|██        | 126/600 [00:06<00:23, 20.21frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 31.0ms\n",
      "Speed: 2.0ms preprocess, 31.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  22%|██▏       | 132/600 [00:06<00:21, 21.35frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 32.0ms\n",
      "Speed: 2.0ms preprocess, 32.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  23%|██▎       | 138/600 [00:07<00:22, 20.41frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 31.0ms\n",
      "Speed: 3.0ms preprocess, 31.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  24%|██▍       | 144/600 [00:07<00:21, 21.07frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 30.0ms\n",
      "Speed: 2.0ms preprocess, 30.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  25%|██▌       | 150/600 [00:07<00:21, 21.03frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 84.0ms\n",
      "Speed: 5.0ms preprocess, 84.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  26%|██▌       | 156/600 [00:08<00:22, 19.34frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 78.0ms\n",
      "Speed: 4.0ms preprocess, 78.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  27%|██▋       | 162/600 [00:08<00:23, 18.84frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 83.0ms\n",
      "Speed: 3.0ms preprocess, 83.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  28%|██▊       | 168/600 [00:08<00:22, 18.92frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 112.0ms\n",
      "Speed: 3.0ms preprocess, 112.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  29%|██▉       | 174/600 [00:09<00:24, 17.50frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  30%|██▉       | 179/600 [00:09<00:21, 19.72frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  31%|███       | 185/600 [00:09<00:19, 21.29frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  32%|███▏      | 191/600 [00:10<00:18, 22.54frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  33%|███▎      | 197/600 [00:10<00:18, 21.80frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  34%|███▍      | 203/600 [00:10<00:17, 22.74frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  35%|███▍      | 209/600 [00:10<00:17, 22.97frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  36%|███▌      | 215/600 [00:11<00:16, 23.14frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 18.0ms\n",
      "Speed: 3.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  37%|███▋      | 221/600 [00:11<00:17, 21.77frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  38%|███▊      | 227/600 [00:11<00:16, 22.83frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  39%|███▉      | 233/600 [00:11<00:15, 23.12frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  40%|███▉      | 239/600 [00:12<00:16, 21.38frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  41%|████      | 245/600 [00:12<00:16, 21.97frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Non_Fall, 28.0ms\n",
      "Speed: 2.0ms preprocess, 28.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  42%|████▏     | 251/600 [00:12<00:15, 22.43frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Fall, 30.0ms\n",
      "Speed: 2.0ms preprocess, 30.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  43%|████▎     | 257/600 [00:13<00:15, 22.25frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Fall, 31.0ms\n",
      "Speed: 2.0ms preprocess, 31.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 00028_H_A_FY_C1.mp4:  44%|████▍     | 263/600 [00:13<00:15, 21.94frame/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m output_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_directory, output_filename)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 비디오 처리 함수 호출\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 처리 완료!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[41], line 19\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(video_path, output_path, model_path)\u001b[0m\n\u001b[0;32m     16\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39mtotal_frames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(video_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m---> 19\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_group = ['best_fall_detection_gru_001_full.pt']\n",
    "\n",
    "# model_group = ['best_fall_detection_gru_001_full.pt', 'best_fall_detection_gru_001_mediapipe.pt',\n",
    "#                'best_fall_detection_gru_0001_simplified.pt', 'best_fall_detection_cnn_mediapipe_0001.pt']\n",
    "\n",
    "video_group = ['D:\\\\Falldown\\\\Dataset\\\\Video_Dataset\\\\Video\\Test\\\\00028_H_A_FY_C1.mp4', #1\n",
    "               'D:\\\\Falldown\\\\Dataset\\\\Video_Dataset\\\\Video\\Test\\\\00130_H_A_FY_C2.mp4', #2\n",
    "               'D:\\\\Falldown\\\\Dataset\\\\Video_Dataset\\\\Video\\Test\\\\00712_H_D_BY_C3.mp4', #3\n",
    "               'D:\\\\Falldown\\\\Dataset\\\\Video_Dataset\\\\Video\\Test\\\\02900_Y_C_BY_C4.mp4', #4\n",
    "               'D:\\\\Falldown\\\\Dataset\\\\Video_Dataset\\\\Video\\Test\\\\02087_H_A_SY_C5.mp4', #5\n",
    "               'D:\\\\Falldown\\\\Dataset\\\\Video_Dataset\\\\Video\\Test\\\\01757_Y_E_SY_C6.mp4', #6\n",
    "               'D:\\\\Falldown\\\\Dataset\\\\Video_Dataset\\\\Video\\Test\\\\00690_H_D_N_C7.mp4', #7\n",
    "               'D:\\\\Falldown\\\\Dataset\\\\Video_Dataset\\\\Video\\Test\\\\00799_O_E_N_C8.mp4'  #8\n",
    "]\n",
    "\n",
    "output_directory = 'D:\\\\Falldown\\\\code-git'\n",
    "\n",
    "# Video 샘플 처리\n",
    "for model_path in model_group:\n",
    "    for video_path in video_group:\n",
    "        suffix = f\"_{model_path.split('_')[3]}_{model_path.split('_')[5].replace('.pt', '')}.mp4\" if '_gru_' in model_path else f\"_{model_path.split('_')[3]}.mp4\" # suffix를 사용하여 각 파일에 따라 구분되는 파일명 만들기\n",
    "        video_name, _ = os.path.splitext(os.path.basename(video_path)) # 비디오 파일명 추출\n",
    "        output_filename = f\"{video_name}{suffix}\"\n",
    "        output_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "        process_video(video_path, output_path, model_path) # 비디오 처리 함수 호출\n",
    "        print(f\"✅ {video_name} 처리 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ed412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
