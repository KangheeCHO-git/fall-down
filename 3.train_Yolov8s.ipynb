{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yaml 파일이 생성되었습니다: D:\\Falldown\\yaml_info_yolov8s.yaml\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 경로 설정 (yolo는 dataset 경로를 images와 labels로 해주어야 인식이 가능)\n",
    "data_root = 'D:\\\\Falldown\\\\Dataset\\\\Resized_Dataset'\n",
    "train_root = f'{data_root}\\\\Train\\\\images'\n",
    "val_root = f'{data_root}\\\\Val\\\\images'\n",
    "test_root = f'{data_root}\\\\Test\\\\images'\n",
    "\n",
    "# 클래스 설정\n",
    "class_names = {0 : 'Human'} # 팀원 yaml에서는 0을 비낙상, 1을 낙상으로 클래스 설정함\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# yaml 설정\n",
    "yaml_info = {\n",
    "    'path' : r'D:\\Falldown', # YOLOv8 기본 경로\n",
    "    'names': class_names,\n",
    "    'nc': num_classes,\n",
    "    'train': train_root,\n",
    "    'val': val_root,\n",
    "    'test': test_root\n",
    "}\n",
    "\n",
    "# YAML 파일 저장 경로\n",
    "yaml_file_path = 'D:\\\\Falldown\\\\yaml_info_yolov8s.yaml'\n",
    "\n",
    "# YAML 파일 생성\n",
    "with open(yaml_file_path, 'w') as f:\n",
    "    yaml.dump(yaml_info, f)\n",
    "\n",
    "print(f'yaml 파일이 생성되었습니다: {yaml_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8s.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.38 available  Update with 'pip install -U ultralytics'\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=D:\\Falldown\\yaml_info_yolov8s.yaml, epochs=50, time=None, patience=30, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cuda, workers=12, project=None, name=human_fall_s2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\human_fall_s2\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\human_fall_s2', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\Falldown\\Dataset\\Resized_Dataset\\Train\\labels... 76130 images, 0 backgrounds, 0 corrupt: 100%|██████████| 76130/76130 [00:30<00:00, 2479.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: D:\\Falldown\\Dataset\\Resized_Dataset\\Train\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\Falldown\\Dataset\\Resized_Dataset\\Val\\labels... 9440 images, 0 backgrounds, 0 corrupt: 100%|██████████| 9440/9440 [00:04<00:00, 1975.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\Falldown\\Dataset\\Resized_Dataset\\Val\\labels.cache\n",
      "Plotting labels to runs\\detect\\human_fall_s2\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 12 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\human_fall_s2\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50      4.09G     0.9372      5.722      1.135         21        640:   1%|▏         | 62/4759 [00:14<18:52,  4.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt: 사용자가 학습을 중단하였습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # 학습 시작 시간 기록\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 학습 실행\n",
    "    result = model.train(\n",
    "        data='D:\\\\Falldown\\\\yaml_info_yolov8s.yaml',\n",
    "        epochs=50,\n",
    "        batch=16,\n",
    "        imgsz=640,\n",
    "        device=device,\n",
    "        workers=12,\n",
    "        amp=True,\n",
    "        patience=30,\n",
    "        name='human_fall_s'\n",
    "    )\n",
    "\n",
    "    # 학습 종료 시간 기록\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"실행 시간: {execution_time:.4f} 초\")  # 약 20시간 소요\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"KeyboardInterrupt: 사용자가 학습을 중단하였습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 11,126,358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\Falldown\\Dataset\\Resized_Dataset\\Val\\labels.cache... 9440 images, 0 backgrounds, 0 corrupt: 100%|██████████| 9440/9440 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 295/295 [01:22<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       9440       9440      0.982      0.661      0.827       0.81\n",
      "              Non_Fall       9440       9440      0.982      0.661      0.827       0.81\n",
      "Speed: 0.2ms preprocess, 5.4ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val\u001b[0m\n",
      "Validation Results:\n",
      "mAP50: 0.8267\n",
      "mAP50-95: 0.8099\n",
      "실행 시간: 114.2418 초\n"
     ]
    }
   ],
   "source": [
    "# 검증 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "# YOLO 모델 로드 및 검증 실행\n",
    "model = YOLO('D:\\\\Falldown\\\\code-git\\\\runs\\\\detect\\\\human_fall_s30\\\\weights\\\\best.pt')\n",
    "val_results = model.val(\n",
    "    data='D:\\\\Falldown\\\\yaml_info_yolov8s.yaml',\n",
    "    imgsz=640,\n",
    "    batch=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# 평가 종료 시간 기록\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Validation Results:\")\n",
    "print(f\"mAP50: {val_results.box.map50:.4f}\")\n",
    "print(f\"mAP50-95: {val_results.box.map:.4f}\")\n",
    "print(f\"실행 시간: {execution_time:.4f} 초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\Falldown\\Dataset\\Resized_Dataset\\Test\\labels... 9580 images, 0 backgrounds, 0 corrupt: 100%|██████████| 9580/9580 [00:04<00:00, 2359.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\Falldown\\Dataset\\Resized_Dataset\\Test\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 300/300 [01:24<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       9580       9580      0.985      0.674      0.833      0.815\n",
      "              Non_Fall       9580       9580      0.985      0.674      0.833      0.815\n",
      "Speed: 0.2ms preprocess, 5.4ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val2\u001b[0m\n",
      "Test Results:\n",
      "mAP50: 0.8332\n",
      "mAP50-95: 0.8146\n",
      "실행 시간: 117.0793 초\n"
     ]
    }
   ],
   "source": [
    "# 테스트 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "# 테스트 실행\n",
    "test_results = model.val(\n",
    "    data='D:\\\\Falldown\\\\yaml_info_yolov8s.yaml',\n",
    "    imgsz=640,\n",
    "    batch=32,\n",
    "    device=device,\n",
    "    split=\"test\"  # Test 데이터셋으로 평가\n",
    ")\n",
    "\n",
    "# 테스트 종료 시간 기록\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# 테스트 결과 출력\n",
    "print(f\"Test Results:\")\n",
    "print(f\"mAP50: {test_results.box.map50:.4f}\")\n",
    "print(f\"mAP50-95: {test_results.box.map:.4f}\")\n",
    "print(f\"실행 시간: {execution_time:.4f} 초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
